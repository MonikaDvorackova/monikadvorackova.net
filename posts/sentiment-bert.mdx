---
title: "When BERT Meets Sentiment: Tackling Imbalanced Text Classification"
date: "2025-08-20"
category: "Applied NLP"
tags:
  - BERT
  - XLM-RoBERTa
  - Sentiment Analysis
  - Imbalanced Data
  - Machine Learning
  - MLflow
  - Weights & Biases
  - Experiment Tracking
  - NLP
tldr: "End-to-end sentiment pipeline: noisy CSV → cleaning & dedup → XLM-RoBERTa fine-tuning with class-weighted loss (max_len 128–256). Decision threshold tuned on VALID. Experiments tracked in W&B/MLflow. Public deliverable: a reusable W&B dashboard you can duplicate and feed with your own runs."
resources:
  - type: github
    href: "https://github.com/monikadvorackova/bert-sentiment"   
  - type: colab
    href: "https://colab.research.google.com/drive/XXXXXXXX"     
    label: "Colab Notebook"
  - type: wandb
    href: "https://wandb.ai/<org>/<project>/reports/<report-id>" 
---

Real-world datasets are rarely as clean as textbook examples. In practice, you often face strong class imbalance — for example, when most user reviews are harmless while only a minority need to be flagged. A trivial classifier that always predicts the majority class can achieve high accuracy, but it fails at the very task it was designed for.  

This project explored how to adapt **XLM-RoBERTa** (a multilingual transformer model) for such a setting.  

## The Setup  

- **Model:** `xlm-roberta-base` fine-tuned for binary classification  
- **Data:** ~80% “approve” vs. 20% “hide”  
- **Challenge:** prevent the model from collapsing into predicting the majority class only  
- **Approach:**  
  - Tokenization with maximum length 256  
  - Cross-entropy loss with **class weights** derived from label distribution  
  - **AdamW optimizer** with weight decay  
  - **Linear scheduler with warmup** to stabilize learning  

## Experiment Tracking  

Modern ML isn’t just about training a model — it’s about knowing *why* it behaves the way it does. To keep the process reproducible and transparent, runs were logged in **MLflow** (artifacts, parameters, metrics) and visualized in **Weights & Biases dashboards**.  

This made it possible to:  
- compare learning curves between experiments,  
- monitor validation accuracy and F1,  
- adjust thresholds and observe their effect on minority-class recall.  

## Key Findings  

- **Validation accuracy** hovered around 0.80, but that number alone was misleading given the imbalance.  
- **Weighted F1** provided a better view: >0.80 on validation.  
- **Minority class (HIDE)** remained the hardest to capture, but threshold tuning improved recall significantly.  
- Logging everything made failures as valuable as successes — one run predicted “HIDE” for nearly everything, highlighting the importance of careful learning rate selection.  

## Reflections  

The interesting part wasn’t just the final number, but the *process*:  
- how to deal with imbalance,  
- which metrics matter beyond accuracy,  
- how experiment tracking tools turn trial-and-error into a reproducible workflow.  

It’s less about building a perfect model and more about building a **scientific practice** around applied NLP.  